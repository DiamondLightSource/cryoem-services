from __future__ import annotations

from typing import Optional

import numpy as np
import pytest

from cryoemservices.util.clem_array_functions import (
    convert_array_dtype,
    estimate_int_dtype,
    flatten_image,
    get_dtype_info,
    get_valid_dtypes,
    merge_images,
)

known_dtypes = (
    "int8",
    "int16",
    "int32",
    "int64",
    "uint8",
    "uint16",
    "uint32",
    "uint64",
    "float16",
    "float32",
    "float64",
    "float128",
    "complex64",
    "complex128",
    "complex256",
)


def test_get_valid_dtypes():
    # All known dtypes should be generated by the function
    assert len(set(known_dtypes) - set(get_valid_dtypes())) == 0


@pytest.mark.parametrize("dtype", known_dtypes)
def test_get_dtype_info(dtype: str):

    # Load dtype info from NumPy
    dtype_info = (
        np.iinfo(dtype) if dtype.startswith(("int", "uint")) else np.finfo(dtype)
    )
    # Load dtype info via function
    dtype_func = get_dtype_info(dtype)

    # Compare the contents of the two objects
    assert (
        len(
            {dtype_info.min, dtype_info.max, str(dtype_info.dtype)}
            - {dtype_func.min, dtype_func.max, str(dtype_func.dtype)}
        )
        == 0
    )


dtype_estimation_test_matrix = (
    # Min value | Max value | Bit depth (optional) | As float? (bool) | Expected estimate
    # Test bit depth parameter
    (-128, 127, 8, True, "int8"),
    (0, 127, 16, False, "uint16"),
    (0, 127, 32, True, "uint32"),
    (-128, 127, 64, False, "int64"),
    # Test auto-find ability
    (0, 127, None, False, "uint8"),
    (-32768, 32767, None, True, "int16"),
    (-2147483648, 2147483647, None, False, "int32"),
    (0, 9223372036854775807, None, True, "uint64"),
)


@pytest.mark.parametrize("test_params", dtype_estimation_test_matrix)
def test_estimate_int_dtype(test_params: tuple[int, int, Optional[int], bool, str]):
    vmin, vmax, bits, is_float, target = test_params

    # Create test array
    shape = (32, 32)
    dtype = "float64" if is_float is True else "int64"
    arr = np.random.randint(vmin, vmax, shape).astype(dtype)

    estimate = estimate_int_dtype(arr, bits)
    assert estimate == target


array_conversion_test_matrix = (
    # Starting dtype | Target dtype | Estimate initial dtype?
    # Float -> int/uint
    ("float64", "int8", True),
    ("float32", "int16", False),
    ("float64", "int32", False),
    ("float32", "int64", True),
    ("float32", "uint8", False),
    ("float64", "uint16", True),
    ("float32", "uint32", True),
    ("float64", "uint64", False),
    # int -> int/uint
    ("int64", "int8", False),
    ("int32", "int16", True),
    ("int16", "int32", False),
    ("int8", "int64", True),
    ("int32", "uint8", True),
    ("int64", "uint16", False),
    ("int8", "uint32", True),
    ("int16", "uint64", False),
    # uint -> int/uint
    ("uint32", "int8", True),
    ("uint64", "int16", False),
    ("uint8", "int32", True),
    ("uint16", "int64", False),
    ("uint64", "uint8", False),
    ("uint32", "uint16", True),
    ("uint16", "uint32", False),
    ("uint8", "uint64", True),
)


@pytest.mark.parametrize("test_params", array_conversion_test_matrix)
def test_convert_array_dtype(test_params: tuple[str, str, bool]):

    def normalize(arr: np.ndarray) -> np.ndarray:
        diff = arr.max() - arr.min()
        return (arr / diff) - (arr.min() / diff)

    # Get dtype parameters
    dtype_0, dtype_1, estimate = test_params
    info_0 = get_dtype_info(dtype_0)
    info_1 = get_dtype_info(dtype_1)
    shape = (16, 16)

    if dtype_0.startswith(("int", "uint")):
        # Use half range of starting array
        # (np.random.randint uses "int64" by default, so "uint64"'s max value will
        # exceed it)
        vmin = int(0.5 * info_0.min)
        vmax = int(0.5 * info_0.max)
        arr_0 = np.random.randint(vmin, vmax, size=shape).astype(dtype_0)
    else:
        # Use half the integer range of target dtype if starting with float
        vmin = int(0.5 * info_1.min)
        vmax = int(0.5 * info_1.max)
        arr_0 = np.random.randint(vmin, vmax, size=shape).astype(dtype_0)

    # Convert the array using the function
    initial_dtype = estimate_int_dtype(arr_0) if estimate is True else None
    arr_1 = convert_array_dtype(arr_0, dtype_1, initial_dtype)

    # Normalise both arrays to (0, 1) for comparison
    arr_0 = normalize(arr_0)
    arr_1 = normalize(arr_1)

    # Check that deviations are within a set threshold:
    # arr_1 - arr_0 = atol + rtol * abs(arr_0)
    np.testing.assert_allclose(
        arr_1,
        arr_0,
        rtol=0,
        atol=0.01,  # <= 1% deviation when going between 64- and 8-bit arrays
    )


def test_stretch_image_contrast():
    pass


def test_convert_to_rgb():
    pass


image_flattening_test_matrix = (
    # Type | Frames | Mode | Is float? | Expected pixel value
    # Test grayscale images
    ("gray", 5, "mean", True, 2),
    ("gray", 5, "min", False, 0),
    ("gray", 5, "max", True, 4),
    # Test RGB images
    ("rgb", 5, "mean", False, 2),
    ("rgb", 5, "min", True, 0),
    ("rgb", 5, "max", False, 4),
)


@pytest.mark.parametrize("test_params", image_flattening_test_matrix)
def test_flatten_image(test_params: tuple[str, int, str, bool, int]):

    # Helper function to create an array simulating an image stack
    def create_test_array(shape: tuple, frames: int, dtype: str) -> np.ndarray:
        for f in range(frames):
            # Increment array values by 1 per frame
            frame = np.ones(shape).astype(dtype) * f
            if f == 0:
                arr = np.array([frame])
            else:
                arr = np.append(arr, [frame], axis=0)
        return arr

    # Unpack test parameters
    img_type, frames, mode, is_float, result = test_params

    # Choose "int" and "float" dtypes
    dtype = "float64" if is_float is True else "int64"

    # Choose between grayscale or RGB image
    if img_type == "gray":
        shape: tuple[int, ...] = (32, 32)
    elif img_type == "rgb":
        shape = (32, 32, 3)
    else:
        raise ValueError("Unexpected value for image type")

    # Create image stack and flatten it
    arr = create_test_array(shape, frames, dtype)
    arr_new = flatten_image(arr, mode)

    # Create new flattened array with the expected pixel value
    #   Because pixel values increase from 0 to (frame - 1) per frame, it's possible
    #   to predict what the array values will be after flattening using "mean", "max",
    #   or "min".
    arr_ref = np.ones(shape).astype(dtype) * result
    # DEBUG: Check that reference array has expected properties
    assert np.all(arr_ref == result) and arr_ref.shape == shape

    # Check that deviations are within a set threshold:
    # arr_1 - arr_0 = atol + rtol * abs(arr_0)
    np.testing.assert_allclose(
        arr_new,
        arr_ref,
        rtol=0,
        atol=1e-20,  # Really, there shouldn't be a difference
    )


image_merging_test_matrix = (
    # Type | No. images | Frames | Is float? | Expected pixel value*
    #   * NOTE: np.round rounds to the nearest EVEN number for values
    #   EXACTLY halfway between rounded decimal values (e.g. 0.5
    #   rounds to 0, -1.5 rounds to -2, etc.).
    # Test grayscale stacks
    ("gray", 1, 5, False, 0),
    ("gray", 2, 5, True, 0.5),
    ("gray", 3, 5, True, 1),
    ("gray", 4, 5, False, 2),
    # Test RGB stacks
    ("rgb", 1, 5, True, 0),
    ("rgb", 2, 5, False, 0),
    ("rgb", 3, 5, False, 1),
    ("rgb", 4, 5, True, 1.5),
    # Test on images
    ("gray", 1, 1, False, 0),
    ("gray", 2, 1, True, 0.5),
    ("rgb", 3, 1, False, 1),
    ("rgb", 4, 1, True, 1.5),
)


@pytest.mark.parametrize("test_params", image_merging_test_matrix)
def test_merge_images(test_params: tuple[str, int, int, bool, int | float]):

    # Unpack test parameters
    img_type, num_imgs, frames, is_float, result = test_params

    # Select dtype
    dtype = "float64" if is_float is True else "int64"

    # Set frame shape based on grayscale or RGB image
    if img_type == "gray":
        shape: tuple[int, ...] = (32, 32)
    elif img_type == "rgb":
        shape = (32, 32, 3)
    else:
        raise ValueError("Unexpected value for image type")

    # Create list of images/stacks and merge them
    arr_list = []
    for n in range(num_imgs):
        # Increment values by image/stack
        arr = np.array([np.ones(shape) for f in range(frames)]).astype(dtype) * n
        arr_list.append(arr)
    # DEBUG: Check that arrays are generated correctly
    assert all(str(type(arr)) == str(np.ndarray) for arr in arr_list)
    composite = merge_images(arr_list)

    # Create a reference stack to compare against
    #   All values for a single image are the same and increment per image from
    #   0 - (num_img -1), so the expected average value of the final product can be
    #   quickly calculated
    arr_ref = np.array([np.ones(shape) for f in range(frames)]).astype(dtype) * result
    # DEBUG: Check that reference array has the expected shape
    assert arr_ref.shape == (frames, *shape)

    # Check that deviations are within a set threshold:
    # arr_1 - arr_0 = atol + rtol * abs(arr_0)
    np.testing.assert_allclose(
        composite,
        arr_ref,
        rtol=0,
        atol=1e-20,  # Really, there shouldn't be a difference
    )


def test_preprocess_img_stk():
    pass


def test_write_stack_to_tiff():
    pass
